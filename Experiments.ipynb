{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiments.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "RUJwxpCA-mfZ",
        "bQMoWbzfqFWz",
        "BX6mVwM2By7O",
        "gLYx_nWLjybi",
        "VrCgfTA7Zm0z",
        "aVna1JHh44qa"
      ],
      "mount_file_id": "1TkcZbc8ki-_B1ByctacaKQgVxoXCkoa9",
      "authorship_tag": "ABX9TyOdP0CVAzMsM8t4+FZAJFqn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUJwxpCA-mfZ"
      },
      "source": [
        "### Package installs and setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwA4KiuYCOxd"
      },
      "source": [
        "!pip install --quiet tensorflow pandas fasttext fsspec tensorflow_text tensorflow keras-tuner transformers gdown gensim\n",
        "!pip install --quiet --upgrade tensorflow-hub\n",
        "!pip install --quiet python-Levenshtein simple_elmo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HEhhtxV-SeI"
      },
      "source": [
        "import re, io, os, math, itertools, statistics, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial import distance\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import kerastuner as kt\n",
        "import tensorflow_hub as hub\n",
        "from Levenshtein import distance as lev_distance\n",
        "#from  IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Drive ids for the directories in the dataset\n",
        "dir_ids = {\n",
        "'COMETA': '',\n",
        "'ZEROSHOT_SPECIFIC': '',\n",
        "'ZEROSHOT_GENERAL': '',\n",
        "'STRATIFIED_GENERAL': '',\n",
        "'STRATIFIED_SPECIFIC':'',\n",
        "}\n",
        "\n",
        "def load_dataset(dir_id):\n",
        "  \"\"\"Load one of the COMETA datasets into a pandas dataframe.\n",
        "  `dir_id` is the Google Drive id.\n",
        "\n",
        "  Returns dictionary in the format {k: v} where k is the filename minus '.csv',\n",
        "  and v is a pandas dataframe e.g. dataset['train'] yields data from 'train.csv'.\n",
        "  \"\"\"\n",
        "  # Get the file ids in the directory\n",
        "  ids = {file1['title']: file1['id']\n",
        "                  for file1 in drive.ListFile({'q': \"'{}' in parents\".format(\n",
        "                      dir_id)}).GetList()\n",
        "        }\n",
        "  \n",
        "  # Load each file in the dataset into pandas\n",
        "  dataset = {}\n",
        "  for k in ids.keys():\n",
        "    key = k[0:-4] # strip out .csv\n",
        "    print(\"Loaded\", key)\n",
        "    file_id = ids[k]\n",
        "    print(file_id) # for some bizarre reason this fixes a weird bug??\n",
        "    content = drive.CreateFile({'id': file_id})\n",
        "    content.GetContentFile(k)\n",
        "    df = pd.read_csv(k, sep='\\t')\n",
        "\n",
        "    dataset[key] = df\n",
        "  \n",
        "  return dataset\n",
        "\n",
        "strat_spec = load_dataset(dir_ids['STRATIFIED_SPECIFIC'])\n",
        "#strat_gen = load_dataset(dir_ids['STRATIFIED_GENERAL'])\n",
        "zsh_spec = load_dataset(dir_ids['ZEROSHOT_SPECIFIC'])\n",
        "#zsh_gen = load_dataset(dir_ids['ZEROSHOT_GENERAL'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQMoWbzfqFWz"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMorHplIWQ_Z"
      },
      "source": [
        "These helper functions are all used with several different classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA9GruAYKQuv"
      },
      "source": [
        "def remove_duplicates(embeddings, series, average=False, preserve=False):\n",
        "  \"\"\"\n",
        "  Remove duplicate embeddings for the labels, preserving order.\n",
        "\n",
        "  Parameters:\n",
        "    embeddings: the embeddings.\n",
        "    series: the corresponding list of strings.\n",
        "    average: If True, average the duplicates. If it is False, just take the\n",
        "    first value for each duplicate.\n",
        "    preserve: If `average` is True and `preserve` is True, then embeddings and\n",
        "    series stay the same size. If `preserve` is False, then the duplicates\n",
        "    get averaged and then only the first value is used.\n",
        "\n",
        "    Don't try to set average=False and preserve=True; you'll get an error.\n",
        "  \n",
        "  Returns a (list1, list2) tuple.\n",
        "    list1: a list of embeddings.\n",
        "    list2: the list of strings, modified if necessary.\n",
        "  \"\"\"\n",
        "  if preserve and not average:\n",
        "    raise ValueError\n",
        "  if embeddings.shape[0] != len(series):\n",
        "    raise ValueError\n",
        "  \n",
        "  if type(series) == list:\n",
        "    l_s = pd.Series(series)\n",
        "    series = l_s\n",
        "\n",
        "  if average: # this makes my brain hurt\n",
        "    duplicates = series.duplicated(keep=False) # mark all dups as True\n",
        "\n",
        "    # Get the indices corresponding to each duplicate label and stick them in\n",
        "    # a dict in the format { 'label': [i, j, k] }\n",
        "    dup_labels_with_indices = {}\n",
        "    for i in range(0, len(duplicates)):\n",
        "      label = series[i]\n",
        "      is_duplicated = duplicates[i]\n",
        "      if not is_duplicated:\n",
        "        continue\n",
        "      elif label not in dup_labels_with_indices:\n",
        "        dup_labels_with_indices[label] = [i]\n",
        "      else:\n",
        "        dup_labels_with_indices[label].append(i)\n",
        "    \n",
        "    # get the embeddings for each duplicate, then average them\n",
        "    dup_labels_with_embeds = {}\n",
        "    for label, indices in dup_labels_with_indices.items():\n",
        "      embeddings = []\n",
        "      for index in indices:\n",
        "        embeddings.append(embeddings[index])\n",
        "      cat = tf.stack(embeddings, axis=0)\n",
        "      avg_embed = tf.reduce_mean(cat, axis=0)\n",
        "      # store the label together with its embedding in a dict\n",
        "      dup_labels_with_embeds[label] = avg_embed\n",
        "    \n",
        "    # now write the avg_embeds, as well as the non-duplicate ones, to a list\n",
        "    embeddings_list = []\n",
        "    for i in range(0, len(duplicates)):\n",
        "      is_duplicate = duplicates[i]\n",
        "      label = series[i]\n",
        "      embed = dup_labels_with_embeds[label] if is_duplicate else embeddings[i]\n",
        "      embeddings_list.append(embed)\n",
        "\n",
        "    # turn it into a tensor\n",
        "    embeddings_tensor = tf.stack(embeddings_list, axis=0)\n",
        "  else:\n",
        "    embeddings_tensor = embeddings # underwhelming\n",
        "\n",
        "  if not preserve: # drop all but the first value of each duplicate\n",
        "    # need ~ so that it returns True for all but the non-first duplicates\n",
        "    mask = ~series.duplicated(keep='first')\n",
        "    masked_labels = series.drop_duplicates(keep='first')\n",
        "    masked_embeds = tf.boolean_mask(embeddings_tensor, mask)\n",
        "    return masked_embeds, masked_labels.to_list()\n",
        "  else: # this retains the duplicates, but with their embeddings avg'd\n",
        "    return embeddings_tensor, series"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ubLmglkaNI4"
      },
      "source": [
        "def create_batches(data, batch_size=100):\n",
        "  \"\"\"Partition a dataset into batches of size batch_size.\"\"\"\n",
        "  num_batches = len(data)//batch_size\n",
        "  \n",
        "  batches = []\n",
        "  for i in range(0, num_batches):\n",
        "    batch = data[batch_size*i:batch_size*(i+1)]\n",
        "    batches.append(batch)\n",
        "  if len(data) % batch_size:\n",
        "    # Deal with the remainder if it exists\n",
        "    batch = data[batch_size*num_batches:]\n",
        "    batches.append(batch)\n",
        "  \n",
        "  return batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wYeqztGLpJs"
      },
      "source": [
        "def pad_outputs(outputs, min_size=None, d=0, batches=False, rank=3):\n",
        "  \"\"\"\n",
        "  Pad a list of outputs to the same size along a specified dimension.\n",
        "\n",
        "  Parameters:\n",
        "    outputs: a list of tensors.\n",
        "    min_size: If None, then pad to the size of the largest tensor.\n",
        "      Otherwise, pad it to min_size (e.g. 64)\n",
        "    d: pad  Pad the inputs on dimension d. Please don't try to use negative indices.\n",
        "      Sadly you need to specify the rank of the tensors you're padding.\n",
        "    batches: if True, do the padding in batches of 25.\n",
        "    rank: the rank of the tensors that are being padded.\n",
        "\n",
        "  Returns a list of outputs, which are all the same size on dimension d.\n",
        "  \"\"\"\n",
        "  # work out the size of the largest batch\n",
        "  output_sizes = [output.shape.as_list() for output in outputs]\n",
        "  max_size = max([shape[d] for shape in output_sizes])\n",
        "\n",
        "  if min_size and max_size > min_size:\n",
        "    raise ValueError(\"You're trying to use a tensor that's too big :(\")\n",
        "  size = min_size if min_size else max_size # size that tensors get padded to\n",
        "\n",
        "  if batches:\n",
        "    output_batches = create_batches(outputs, batch_size=25)\n",
        "    padded_outputs_b = []\n",
        "    for batch in output_batches:\n",
        "      padded_outputs_b.append(pad_outputs(batch, min_size=size, batches=False))\n",
        "    # collapse 2d list into 1d\n",
        "    padded_outputs = list(itertools.chain.from_iterable(padded_outputs_b))\n",
        "  \n",
        "  else:\n",
        "    # now pad all the batches so they are of the same size\n",
        "    padded_outputs = []\n",
        "    for i in range(0,len(outputs)):\n",
        "      output = outputs[i]\n",
        "      pad = size - output.shape.as_list()[d]\n",
        "\n",
        "      if d == 0:\n",
        "        paddings = [[0, pad],]\n",
        "      elif d == 1:\n",
        "        paddings = [[0, 0], [0, pad],]\n",
        "      # add the remaining [0,0] values onto the end of the paddings\n",
        "      paddings.extend([[0,0]*1 for _ in range(rank-d-1)])\n",
        "      \n",
        "      padded = tf.pad(output, paddings, 'CONSTANT')\n",
        "      padded_outputs.append(padded)\n",
        "  \n",
        "  return padded_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TQf20BfSHxR"
      },
      "source": [
        "class Results():\n",
        "  \"\"\"\n",
        "  A class that acts as a wrapper around a pandas df that contains the results of\n",
        "  an experiment.\n",
        "\n",
        "  Parameters:\n",
        "    df: a datafile with headers 'Term', 'Gold label', 'Correct'\n",
        "    meta: a list in the format [in_name, in_d, out_name, out_d, k]\n",
        "    use_ids: determines whether the output is a SNOMED id, in which case several\n",
        "      of the metrics become nonsensical.\n",
        "  \"\"\"\n",
        "  def __init__(self, df, meta, use_ids):\n",
        "    self.data = df\n",
        "    self.meta = meta\n",
        "    self.use_ids = use_ids\n",
        "\n",
        "  def __exclude_easy(self):\n",
        "    \"\"\"Remove the easy cases where term==label from the dataset.\"\"\"\n",
        "    return self.data[(self.data['Term'].apply(lower) != self.data['Gold label'].apply(lower))]\n",
        "\n",
        "  def __get_correct(self, exclude_easy=True):\n",
        "    \"\"\"\n",
        "    Get the times that the experiment actually got the answer right.\n",
        "    \n",
        "    Parameters:\n",
        "      exclude_easy: controls whether to exclude cases where the Term and Gold label\n",
        "      are identical.\n",
        "    \n",
        "    Returns a datafile of the correct results.\n",
        "    \"\"\"\n",
        "    if exclude_easy:\n",
        "      correct = self.data[(self.data['Correct'] == True) &\n",
        "                          (self.data['Term'].apply(lower) !=\n",
        "                           self.data['Gold label'].apply(lower))]\n",
        "    else:\n",
        "      correct = self.data[self.data['Correct'] == True]\n",
        "    \n",
        "    return correct\n",
        "  \n",
        "  def __get_edit_distance(self):\n",
        "    \"\"\"\n",
        "    Get the Levenshtein edit distance between each term and each label.\n",
        "    Both term and label are lowercase.\n",
        "\n",
        "    Modifies the datafile in place.\n",
        "    \"\"\"\n",
        "    edit_distances = []\n",
        "    for i in range(len(self.data['Term'])):\n",
        "      term = self.data['Term'][i].lower()\n",
        "      label = self.data['Gold label'][i].lower()\n",
        "\n",
        "      edit_distances.append(lev_distance(term, label))\n",
        "    \n",
        "    self.data['Edit distance'] = edit_distances\n",
        "\n",
        "  def analyse(self):\n",
        "    \"\"\"\n",
        "    Analyse the results. Returns a list of five numbers:\n",
        "    - the total number of right answers\n",
        "    - the total number of attempts\n",
        "    - the total number of right answers, excluding the easy ones\n",
        "    - the total number of attempts, excluding the easy ones\n",
        "    - for the right answers, the average Levenshtein edit distance between\n",
        "      the term and the label normalised by adding 1; this means that if\n",
        "      Correct=True and Edit distance=0, i.e. if the term is the same as the gold\n",
        "      label, the value of Correct*ED = 1. In other words, the algorithm gets\n",
        "      credit for picking off the lowest hanging fruit.\n",
        "    - the same, but not normalised. Here, if Correct=True and Edit distance=0,\n",
        "      the value of Correct*ED = 0. It is therefore an exclude easy option.\n",
        "    \"\"\"\n",
        "    self.__get_edit_distance()\n",
        "\n",
        "    correct_all = self.__get_correct(exclude_easy=False).shape[0]\n",
        "    all_size = self.data.shape[0]\n",
        "    # If the output is an id, the text-similarity based metrics are nonsensical\n",
        "    #if not self.use_ids:\n",
        "    correct_ee = self.__get_correct(exclude_easy=True).shape[0]\n",
        "    ee_size = self.__exclude_easy().shape[0]\n",
        "  \n",
        "    avg_edit_norm = sum(self.data['Correct']*(self.data['Edit distance']+1))/correct_all\n",
        "    return [correct_all, all_size, correct_ee, ee_size, avg_edit_norm]\n",
        "    #else:\n",
        "      # Empty strings so that the dataset stays the same size\n",
        "      #return [correct_all, all_size, \"\", \"\", \"\"]\n",
        "  \n",
        "  def __str__(self):\n",
        "    # if not self.use_ids:\n",
        "    s = \"\"\"{} (d={}) -> {} (d={})\n",
        "    k={}\n",
        "    Correct all: {} / {} \n",
        "    Correct exclude easy: {} / {}\n",
        "    Avg normalised edit distance for the right answers: {}\n",
        "\n",
        "    \"\"\".format(*self.meta, *self.analyse())\n",
        "    # else:\n",
        "    #   s = \"\"\"{} (d={}) -> {} (d={})\n",
        "    #   k={}\n",
        "    #   Correct all: {} / {}\n",
        "\n",
        "    #   \"\"\".format(*self.meta, self.analyse()[0], self.analyse()[1])\n",
        "    \n",
        "    return s+\"\\t\".join([str(m) for m in self.meta] + [str(a) for a in self.analyse()])\n",
        "\n",
        "def lower(s):\n",
        "  return s.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRFqJIrvr1W9"
      },
      "source": [
        "class Experiment():\n",
        "  \"\"\"A class for an experiment, whether linear or neural.\"\"\"\n",
        "  \n",
        "  def run(self, zsh=False):\n",
        "    \"\"\"\n",
        "    Run an experiment over the whole dataset.\n",
        "    Generate the embeddings and learn the transformation between\n",
        "    train-term and train-label.\n",
        "    Make a prediction for each test-term's label, and see if it was correct.\n",
        "\n",
        "    Parameters:\n",
        "      zsh: if this is True, then compare the predictions against the labels known\n",
        "      to be in the test set. You need to use this for the zeroshot datasets!\n",
        "      Otherwise, compare against the labels that were seen in the train set. It\n",
        "      shouldn't make a substantive difference with the stratified datasets.\n",
        "    \"\"\"\n",
        "    self.create_mapping() # generate the input embeds and learn the mapping\n",
        "    self.make_predictions() # make predictions\n",
        "    \n",
        "    # Get rid of the duplicates in the labels/ids and the corresponding input embeds\n",
        "    if not zsh:\n",
        "      # Compare against the labels known to be in the train set (default)\n",
        "      out_embs_unique, snomed = remove_duplicates(self.y_train, self.train_out)\n",
        "    else:\n",
        "      # Compare against the labels known to be in the test set\n",
        "      out_embs_unique, snomed = remove_duplicates(self.y_test, self.test_out)\n",
        "\n",
        "    results_list = [[] for i in range(0,len(self.k_values))]\n",
        "    for i in range(0, len(self.test_in)):\n",
        "      term = self.test_in[i]\n",
        "      gold = self.test_out[i]\n",
        "      prediction = self.predictions[i]\n",
        "\n",
        "      if self.use_ids:\n",
        "        gold_id = gold\n",
        "        gold_label = self.test_out_other[i]\n",
        "      else:\n",
        "        gold_label = gold\n",
        "        gold_id = self.test_out_other[i]\n",
        "\n",
        "      # Get the distances between the prediction and each label embedding\n",
        "      distances = distance.cdist([prediction], out_embs_unique, \"cosine\")[0]\n",
        "      # distances = distance.cdist([prediction], self.snomed_embeds, \"cosine\")[0]\n",
        "      # get the indices that would sort the distances array\n",
        "      top_indices = np.argsort(distances)\n",
        "\n",
        "      # test the model against each value of k\n",
        "      for i in range(0, len(self.k_values)):\n",
        "        k = self.k_values[i]\n",
        "        possibilities = [snomed[i] for i in top_indices[0:k]]\n",
        "\n",
        "        val = True if gold in possibilities else False\n",
        "        results_list[i].append([term, gold_label, gold_id, val])\n",
        "\n",
        "    # make a Results object for each value of k\n",
        "    self.results = []\n",
        "    col_names = ['Term', 'Gold label', 'Gold id', 'Correct']\n",
        "    for i in range(0, len(self.k_values)):\n",
        "      k = self.k_values[i]\n",
        "      df = pd.DataFrame(results_list[i], columns=col_names)\n",
        "      meta = [self.in_name, self.in_model.d, self.out_name, self.out_model.d, k]\n",
        "      self.results.append(Results(df, meta, self.use_ids))\n",
        "    \n",
        "    return self.results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTdyfHAUzRbo"
      },
      "source": [
        "class Experiments():\n",
        "  \"\"\"\n",
        "  A class to represent several linear or neural mapping experiments run with\n",
        "  several different in_models and several different values of k. They should all\n",
        "  use the same datasets.\n",
        "  \"\"\"\n",
        "  def run(self, zsh=False):\n",
        "    \"\"\"Run all the experiments.\"\"\"\n",
        "    self.results_data = []\n",
        "    for exp in self.experiments:\n",
        "      results = exp.run(zsh)\n",
        "      for r in results:\n",
        "        print(r)\n",
        "        if exp.description != 'linear':\n",
        "          exp.metrics()\n",
        "        print()\n",
        "        self.results_data.append([*r.meta, *r.analyse(), *exp.metrics(), exp.description])\n",
        "\n",
        "    df = pd.DataFrame(self.results_data, columns=self.header)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQLhSZoOfrfy"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im0w9dHr4swt"
      },
      "source": [
        "# The headers are \n",
        "# ['ID', 'Term', 'General SNOMED Label', 'General SNOMED ID',\n",
        "# 'Specific SNOMED Label', 'Specific SNOMED ID', 'Example', 'Subreddit']\n",
        "\n",
        "# Stop truncating data in columns\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "class Dataset():\n",
        "  \"\"\"\n",
        "  A dataset that the experiment is run on.\n",
        "  Mostly a wrapper for a dataframe.\n",
        "\n",
        "  Arguments:\n",
        "    df: the pandas dataframe.\n",
        "    kind: whether to use the general or specific SNOMED labels. Make sure to use\n",
        "    general with the strat_gen or zsh_gen dataset, and mutatis mutandis for spec.\n",
        "  \"\"\"\n",
        "  def __init__(self, df, kind='specific'):\n",
        "    self.df = df\n",
        "    self.kind=kind\n",
        "    self.cometa_ids = [str(id) for id in self.df['ID'].to_list()]\n",
        "    self.labels = self.df[self.kind.capitalize()+' SNOMED Label'].to_list()\n",
        "    self.ids = [str(id) for id in self.df[self.kind.capitalize()+' SNOMED ID'].to_list()]\n",
        "    self.terms = self.df['Term'].to_list()\n",
        "    self.examples = self.df['Example'].to_list()\n",
        "    self.subreddits = self.df['Subreddit'].to_list()\n",
        "    self.size = len(self.labels)\n",
        "  \n",
        "  def __getitem__(self, key):\n",
        "    \"\"\"Subscripting returns the ith row of the dataset.\"\"\"\n",
        "    return self.df.iloc[key, :]\n",
        "\n",
        "  def col(self, name):\n",
        "    \"\"\"Returns a list of the values in a particular column.\"\"\"\n",
        "    return [str(i) for i in self.df[name].to_list()]\n",
        "  \n",
        "  def unique_subreddits(self):\n",
        "    \"\"\"Returns a list of the unique subreddits that the data was extracted from.\"\"\"\n",
        "    return self.df['Subreddit'].unique()\n",
        "  \n",
        "  def unique_labels(self):\n",
        "    \"\"\"Returns a list of the unique SNOMED labels.\"\"\"\n",
        "    return self.df[self.kind.capitalize()+' SNOMED Label'].unique()\n",
        "  \n",
        "  def edit_distances(self):\n",
        "    \"\"\"\n",
        "    Get the Levenshtein edit distance between each term and each label.\n",
        "    Both term and label are lowercase.\n",
        "\n",
        "    Returns a dictionary of the distribution.\n",
        "    \"\"\"\n",
        "    label_col = self.kind.capitalize() + ' SNOMED Label'\n",
        "\n",
        "    edit_distances = {}\n",
        "    for i in range(len(self.df['Term'])):\n",
        "      term = self.df['Term'][i].lower()\n",
        "      label = self.df[label_col][i].lower()\n",
        "      lev = lev_distance(term, label)+1 # normalise by adding 1\n",
        "      \n",
        "      if lev in edit_distances:\n",
        "        edit_distances[lev] += 1\n",
        "      else:\n",
        "        edit_distances[lev] = 1\n",
        "\n",
        "    return edit_distances.keys(), edit_distances.values()\n",
        "  \n",
        "  def __cos_sim(self, v1, v2):\n",
        "    \"\"\"Returns the cosine similarity between two vectors, or 0 if either vector\n",
        "    is 0.\"\"\"\n",
        "    if not tf.math.count_nonzero(v1) or not tf.math.count_nonzero(v2):\n",
        "      return 0.0\n",
        "    else:\n",
        "      return -distance.cosine(v1, v2)+1\n",
        "\n",
        "  def avg_cos_sim1(self, model):\n",
        "    \"\"\"\n",
        "    Returns the average cosine similarity between terms and labels, both lowercased.\n",
        "\n",
        "    Arguments:\n",
        "      model: the static or dynamic model used to find the cosine similarity.\n",
        "    \"\"\"\n",
        "    terms = [t.lower() for t in self.terms]\n",
        "    labels = [l.lower() for l in self.labels]\n",
        "    embeds = model.get_embeddings(terms, labels, pooled=True)\n",
        "\n",
        "    # similarities = [\n",
        "    #                 -distance.cosine(embeds[0][i], embeds[1][i])+1 for i\n",
        "    #                 in range(0,len(terms))\n",
        "    # ]\n",
        "    similarities = [self.__cos_sim(embeds[0][i], embeds[1][i]) for i\n",
        "                    in range (0,len(terms))]\n",
        "\n",
        "    return np.mean(similarities)\n",
        "\n",
        "  def avg_cos_sim2(self, col, model, k=100):\n",
        "    \"\"\"\n",
        "    Returns the average cosine distance between each pair in a random sample\n",
        "    of size k of a single column.\n",
        "\n",
        "    Parameters:\n",
        "      col: the column to take the sample from.\n",
        "      model: the model to process the sample with.\n",
        "      k: the size of the sample. DO NOT MAKE THIS TOO BIG.\n",
        "    \n",
        "    Returns:\n",
        "      The average cosine similarity between each pair.\n",
        "    \"\"\"\n",
        "    samples = random.sample(range(0,self.size), k=k)\n",
        "    if col == 'terms':\n",
        "      col_embeds = model.process_dataset([self.terms[i] for i in samples])\n",
        "    elif col == 'labels':\n",
        "      col_embeds = model.process_dataset([self.labels[i] for i in samples])\n",
        "    elif col == 'ids':\n",
        "      col_embeds = model.process_dataset([self.ids[i] for i in samples])\n",
        "    \n",
        "    # combs is all the (col[i], col[j]) pairs in column col, where i != j\n",
        "    # it is thus a tensor of shape (T(self.size-1), 2, model.d), where T(n) is\n",
        "    # the nth triangle number\n",
        "    col_embeds = col_embeds.numpy().tolist()\n",
        "    combs = list(itertools.combinations(col_embeds, 2))\n",
        "    combs = tf.constant(combs)\n",
        "    \n",
        "    # # calculate the cosine similarity between each pair in combs\n",
        "    batches = create_batches(combs, batch_size=100)\n",
        "    cos_sims = [[self.__cos_sim(comb[0],comb[1]) for comb in batch] for batch in batches]\n",
        "    cos_sims = tf.concat(cos_sims, axis=-1)\n",
        "    return float(tf.reduce_mean(cos_sims))\n",
        "    \n",
        "  def sample(self, static_model, k=100):\n",
        "    \"\"\"\n",
        "    Get a random sample of size k of the entries in the dataset.\n",
        "    \n",
        "    Arguments:\n",
        "      static_model: the static model to use to get the cosine similarity between\n",
        "      the label and the term.\n",
        "\n",
        "      k: the size of the random sample\n",
        "    \n",
        "    Returns:\n",
        "      A dataframe with headers ['COMETA ID', 'Term', 'Label', 'lev', 'cos_sim'].\n",
        "      'lev' contains the Levenshtein edit distance between 'Term' and 'Label'.\n",
        "      'cos_sim' contains the cos similarity computed by static_model between\n",
        "      'Term and 'Label'.\n",
        "    \"\"\"\n",
        "    sample = []\n",
        "    samples = random.sample(range(0,self.size), k=k)\n",
        "    for i in samples:\n",
        "      cometa_id = self.cometa_ids[i]\n",
        "      label = self.labels[i]\n",
        "      term = self.terms[i]\n",
        "\n",
        "      lev = lev_distance(label.lower(), term.lower())\n",
        "\n",
        "      label_embed = static_model.process_string(label.lower(), pooled=True)\n",
        "      term_embed = static_model.process_string(term.lower(), pooled=True)\n",
        "      # why do you have to do the - and +1? No clue.\n",
        "      cos_sim = self.__cos_sim(label_embed, term_embed)\n",
        "\n",
        "      sample.append([cometa_id, term, label, lev, round(cos_sim, 3)])\n",
        "    \n",
        "    cols = ['COMETA ID', 'Term', 'Label', 'lev', 'cos_sim']\n",
        "    return pd.DataFrame(sample, columns=cols)\n",
        "\n",
        "  def term_stats(self):\n",
        "    \"\"\"\n",
        "    Returns a list of the unique terms in the dataset, and a list of their counts.\n",
        "    \"\"\"\n",
        "    stats = {}\n",
        "    for term in self.terms:\n",
        "      index = len(term.split())\n",
        "      if index in stats:\n",
        "        stats[index] += 1\n",
        "      else:\n",
        "        stats[index] = 1\n",
        "    return stats.keys(), stats.values()\n",
        "  \n",
        "  def join(self, order='l e'):\n",
        "    \"\"\"\n",
        "    Join the labels and terms/examples together. This is to try to get\n",
        "    context-sensitive label embeddings with BERT.\n",
        "\n",
        "    Arguments:\n",
        "      order controls whether the join goes label-example (l e) or example-label\n",
        "      (e l) or label-term (l t) or term-label (t l).\n",
        "      Insert a full stop (e.g. `l. t`) to add a [SEP] token.\n",
        "    \n",
        "    Returns:\n",
        "      A list of the joined labels and terms/examples.\n",
        "    \"\"\"\n",
        "    sep = order[1]\n",
        "    ord = order[0] + order[2]\n",
        "    if ord == 'le':\n",
        "      return [self.labels[i]+sep+\" \"+self.examples[i] for i in range(0,self.size)]\n",
        "    elif ord == 'el':\n",
        "      return [self.examples[i]+sep+\" \"+self.labels[i] for i in range(0,self.size)]\n",
        "    elif ord == 'lt':\n",
        "      return [self.labels[i]+sep+\" \"+self.terms[i] for i in range(0,self.size)]\n",
        "    elif ord == 'tl':\n",
        "      return [self.terms[i]+sep+\" \"+self.labels[i] for i in range(0,self.size)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8XUpUL0MtFv"
      },
      "source": [
        "models = [ft_br, glove_br, bert_br]\n",
        "for m in models:\n",
        "  sim_strat = statistics.mean([test_d.avg_cos_sim2('terms', m, k=100) for i in range(0, 10)])\n",
        "  sim_zsh = statistics.mean([test_d_zsh.avg_cos_sim2('terms', m, k=100) for i in range(0, 10)])\n",
        "  print(m.name+\"\\t\"+str(sim_strat)+\"\\t\"+str(sim_zsh))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad0wqJCU6WkG"
      },
      "source": [
        "train_d = Dataset(strat_spec['train'], kind='specific')\n",
        "val_d = Dataset(strat_spec['dev'], kind='specific')\n",
        "test_d = Dataset(strat_spec['test'], kind='specific')\n",
        "\n",
        "train_d_zsh = Dataset(zsh_spec['train'], kind='specific')\n",
        "val_d_zsh = Dataset(zsh_spec['dev'], kind='specific')\n",
        "test_d_zsh = Dataset(zsh_spec['test'], kind='specific')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX2Ryhy8i04y"
      },
      "source": [
        "ft_br = loaded_ft_models['reddit-biomed']\n",
        "# ft_cc = loaded_ft_models['crawl-300d-2M-subword']\n",
        "# ft_wiki = loaded_ft_models['wiki-news-300d-1M-subword']\n",
        "glove_br = loaded_glove_models['bioreddit.glove.200']\n",
        "# glove_cc = loaded_glove_models['glove.42B.300d']\n",
        "# glove_wiki = loaded_glove_models['glove.6B.200d']\n",
        "bert_br = BertModel(bert_tokenizer, bert_model, 'bert_br', 'lasths')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CioK5Qq3hjW"
      },
      "source": [
        "def get_edit_dist_stats(datasets):\n",
        "  pd.set_option(\"display.max_rows\", None)\n",
        "  edit_dists = []\n",
        "  for dataset in datasets:\n",
        "    eds = dataset.edit_distances()\n",
        "    df = pd.DataFrame(eds[1], index=eds[0])\n",
        "    edit_dists.append(df)\n",
        "\n",
        "  return pd.concat(edit_dists, axis=1, join='outer')\n",
        "\n",
        "get_edit_dist_stats([train_d, val_d, test_d, train_d_zsh, val_d_zsh, test_d_zsh])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fouNXpi--BK-"
      },
      "source": [
        "def get_term_length_stats(datasets):\n",
        "  stats = []\n",
        "  for dataset in datasets:\n",
        "    s = dataset.term_stats()\n",
        "    df = pd.DataFrame(s[1], index=s[0])\n",
        "    stats.append(df)\n",
        "  return pd.concat(stats, axis=1, join='outer')\n",
        "\n",
        "get_term_length_stats([train_d, val_d, test_d, train_d_zsh, val_d_zsh, test_d_zsh])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z00SYIf_BlJo"
      },
      "source": [
        "### Neural input models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF6Hx7noBhpQ"
      },
      "source": [
        "from transformers import TFAutoModel, AutoTokenizer, BertTokenizer\n",
        "\n",
        "MODEL_NAME = \"cambridgeltl/BioRedditBERT-uncased\" # @param {type: \"string\"} [\"bert-base-uncased\", \"bert-large-uncased\", \"cambridgeltl/BioRedditBERT-uncased\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"]\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "bert_model = TFAutoModel.from_pretrained(MODEL_NAME)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsbLR5-27bkw"
      },
      "source": [
        "# Download elmo bioreddit\n",
        "# !mkdir elmo-br\n",
        "# !wget https://github.com/basaldella/bioreddit/releases/download/v1.0/bioreddit.elmo.json\n",
        "# !mv bioreddit.elmo.json elmo-br/options.json\n",
        "# !wget https://github.com/basaldella/bioreddit/releases/download/v1.0/bioreddit.elmo.hdf5\n",
        "# !mv bioreddit.elmo.hdf5 elmo-br/model.hdf5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuNWeeM5VdIw"
      },
      "source": [
        "class NeuralModel():\n",
        "  \"\"\"\n",
        "  A wrapper for methods common to all the neural models.\n",
        "  \"\"\"\n",
        "  model_type = 'neural'\n",
        "  \n",
        "  def process_dataset(self, dataset, pooled=True, batch_size=100):\n",
        "    \"\"\"\n",
        "    Process a whole dataset, batch by batch, then concatenate the batch output.\n",
        "    \n",
        "    Arguments:\n",
        "      dataset: a list of strings. This is the dataset to be processed by the\n",
        "      neural model.\n",
        "      pooled: whether to average the output. Default True.\n",
        "      batch_size: the data is processed in batches of this size. Default 100.\n",
        "    \n",
        "    Returns a Tensor of dynamic embeddings for the dataset.\n",
        "    \"\"\"\n",
        "    if self.name == 'ELMo-br-1024':\n",
        "      # it is far quicker to do the whole batch in one go with the simple_elmo model\n",
        "      return self.process_batch(dataset, pooled=pooled)\n",
        "    \n",
        "    else:\n",
        "      batches = create_batches(dataset, batch_size=batch_size)\n",
        "      outputs = [self.process_batch(batch, pooled=pooled)\n",
        "          for batch in batches]\n",
        "      \n",
        "      print(\"Processed {} batches and {} examples\".format(len(batches), len(dataset)))\n",
        "      # No need to pad as the outputs are all the same dimensions if pooled\n",
        "      # else pad the outputs in batches of 25\n",
        "      padded_outputs = outputs if pooled else pad_outputs(outputs, min_size=None, batches=False, d=1)\n",
        "      return tf.concat(padded_outputs, 0)\n",
        "    \n",
        "  def get_embeddings(self, *datasets, pooled=False, pad_size=None):\n",
        "    \"\"\"\n",
        "    Get the neural embeddings for a list of datasets.\n",
        "    \n",
        "    Parameters:\n",
        "      datasets: a nonzero number of Dataset objects to be processed.\n",
        "      pooled: whether to average the embeddings for multi-word inputs.\n",
        "      pad_size for compatibility.\n",
        "\n",
        "    Returns a list of tensors, one tensor for each Dataset.\n",
        "    \"\"\"\n",
        "    embeddings = [self.process_dataset(ds,pooled=pooled) for ds in datasets]\n",
        "    return embeddings\n",
        "\n",
        "  def get_cs_embeddings(self, *datasets, order='l t', pooled=True, batch_size=100):\n",
        "    \"\"\"\n",
        "    Process a series of datasets, batch by batch. For each element in each batch,\n",
        "    extract the context-sensitive features corresponding to a given string.\n",
        "\n",
        "    Parameters:\n",
        "      datasets: a nonzero number of datasets to process.\n",
        "      order: (Put a . between l and e for a separator)\n",
        "        'l e': label then example. Extract the label.\n",
        "        'e l': example then label. Extract the label.\n",
        "        'l t': label then term. Extract the label.\n",
        "        't l': term then label. Extract the label.\n",
        "        'term': Extract the term from the example.\n",
        "      pooled: If you are using 'term', you can optionally set pooled to False,\n",
        "      which gives a separate, context-sensitive embedding for each word of the term.\n",
        "    \n",
        "    Returns a tensor of dynamic, context-sensitive embeddings.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    for dataset in datasets:\n",
        "      if order[-5:] == '-term':\n",
        "        terms = dataset.terms\n",
        "        examples = dataset.examples\n",
        "        position = 'mid'\n",
        "        pass\n",
        "\n",
        "      else: # we're extracting the labels here\n",
        "        sep = True if order[1] == '.' else False\n",
        "\n",
        "        labels = dataset.labels\n",
        "         # concatenate the labels and examples\n",
        "        joined = dataset.join(order=order)\n",
        "\n",
        "        joined_batches = create_batches(joined, batch_size=batch_size)\n",
        "        label_batches = create_batches(labels, batch_size=batch_size)\n",
        "\n",
        "        # where is the label that we want to extract\n",
        "        position = 'start' if order in ['l e', 'l t', 'l.e', 'l.t'] else 'end'\n",
        "        \n",
        "        # extract the embedding corresponding to each label\n",
        "        label_outputs = [self.cs_process_batch(joined_batches[i], label_batches[i], pos=position, sep=sep) for\n",
        "                        i in range(0,len(joined_batches))]\n",
        "        print(\"Processed {} batches and {} examples\".format(len(label_batches), len(labels)))\n",
        "\n",
        "        # concatenate, remove duplicates and concatenate again\n",
        "        rd, labs = remove_duplicates(tf.concat(label_outputs, axis=0), labels, average=True, preserve=True)\n",
        "        embeddings.append(tf.concat(rd,axis=0))\n",
        "    \n",
        "    return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXZ8PI23NTB_"
      },
      "source": [
        "class BertModel(NeuralModel):\n",
        "  \"\"\"\n",
        "  A BERT model.\n",
        "\n",
        "  Parameters:\n",
        "    tokenizer: the BERT tokeniser.\n",
        "    model: the BERT model.\n",
        "    name: the name that is listed in the table of results.\n",
        "    kind: one of the following values that determine how you get the\n",
        "    individual word vectors:\n",
        "      (see https://towardsdatascience.com/breaking-bert-down-430461f60efb, at the bottom)\n",
        "      avgall: average all the hidden layers together\n",
        "      lasths: last hidden state\n",
        "      sumlast4: sum the last four hidden layers\n",
        "      catlast4: concatenate the last four layers\n",
        "  \"\"\"\n",
        "  def __init__(self, tokenizer, model, name, kind):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.model = model\n",
        "    self.kind = kind\n",
        "    self.d = self.model.config.hidden_size\n",
        "    self.d *= 4 if self.kind == 'catlast4' else 1\n",
        "    self.name = name+'-'+str(self.d)+'-'+self.kind\n",
        "  \n",
        "  def process_batch(self, batch, pooled=True):\n",
        "    \"\"\"\n",
        "    Process a single batch with BERT.\n",
        "\n",
        "    Arguments:\n",
        "      batch: a list of strings to process.\n",
        "      pooled: average the word vectors.\n",
        "    \n",
        "    Returns:\n",
        "      if pooled, a tensor of shape (batch_size, d)\n",
        "      if not pooled, shape (batch_size, max_length, d)\n",
        "        d=768, unless model.kind == 'catlast4', in which case d=3072.\n",
        "    \"\"\"\n",
        "    tokenized = self.tokenizer(batch, return_tensors='tf',\n",
        "                               padding=True, truncation=True, max_length=512)\n",
        "    # output_type = 'pooler_output' if pooled else 'last_hidden_state'\n",
        "    hidden_states = self.model.call(tokenized, output_hidden_states=True, return_dict=True)['hidden_states']\n",
        "    \n",
        "    if self.kind == 'lasths':\n",
        "      unpooled = hidden_states[-1]\n",
        "\n",
        "    if self.kind == 'avgall':\n",
        "      # get a tensor of dimensions (batch_size, 12, max_length, 768)\n",
        "      stacked = tf.stack(hidden_states[1:], axis=1)\n",
        "      # get a tensor of dimensions (batch_size, max_length, 768)\n",
        "      unpooled = tf.reduce_mean(stacked, axis=1)\n",
        "\n",
        "    elif self.kind == 'sumlast4':\n",
        "      stacked = tf.stack(hidden_states[-4:], axis=1)\n",
        "      unpooled = tf.reduce_sum(stacked, axis=1)\n",
        "    \n",
        "    elif self.kind == 'catlast4':\n",
        "      # get a tensor of dimensions (batch_size, max_length, 3072)\n",
        "      unpooled = tf.concat(hidden_states[-4:], axis=-1)\n",
        "    \n",
        "    # compatibility with ... something\n",
        "    unpooled = tf.cast(unpooled, tf.float64)\n",
        "    return tf.reduce_mean(unpooled, axis=-2) if pooled else unpooled\n",
        "  \n",
        "  def cs_process_batch(self, batch, sbatch, pos='start', sep=True):\n",
        "    \"\"\"\n",
        "    Process a single batch with BERT. Then, for each member of the batch m[i],\n",
        "    extract the features corresponding to a given string in sbatch s[i].\n",
        "\n",
        "    The features are just those from the final hidden layer; see\n",
        "    http://jalammar.github.io/illustrated-bert/, 'BERT for feature extraction'.\n",
        "\n",
        "    Parameters:\n",
        "      batch: a list of strings whose representations will be extracted.\n",
        "      sbatch: a list of strings, where sbatch[i] contains batch[i]\n",
        "      pos: controls where s[i] begins within each value of m[i]\n",
        "        'start': beginning\n",
        "        'end': end\n",
        "        'mid': searches through m[i] for the first occurrence of s[i] (use this\n",
        "          one for getting the term out of the example)\n",
        "      sep=True if a full stop separates s[i] from the rest of the string.\n",
        "    \n",
        "    Returns:\n",
        "      A tensor of shape (batch_size, d)\n",
        "    \"\"\"\n",
        "    self.kind == 'lasths'\n",
        "\n",
        "    if len(batch) != len(sbatch):\n",
        "      raise ValueError('You need to give the same number of strings to extract as inputs.')\n",
        "\n",
        "    if pos in ['start', 'end']:\n",
        "      outputs = self.process_batch(batch, pooled=False)\n",
        "      stokens = [self.tokenizer.tokenize(s) for s in sbatch]\n",
        "      \n",
        "      # note that it is irrelevant whether there is a [SEP] token in the middle.\n",
        "      # as you never have to extract the [SEP] token.\n",
        "      if pos == 'start':\n",
        "        # Extract the features corresponding to the string, then average them to\n",
        "        # produce a single embedding for the string\n",
        "        # a [CLS] token is prepended to the string, hence +1\n",
        "        sfeatures = [tf.reduce_mean(outputs[i][0+1:len(stokens[i])+1],axis=0) for\n",
        "                    i in range(0, len(batch))]\n",
        "      elif pos == 'end':\n",
        "        # a [SEP] token is appended to the string, hence the -1\n",
        "        sfeatures = [tf.reduce_mean(outputs[i][-1-len(stokens[i]):-1],axis=0) for\n",
        "                    i in range(0, len(batch))]\n",
        "    \n",
        "    elif pos == 'mid':\n",
        "      pass\n",
        "\n",
        "    return tf.stack(sfeatures,0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRMh8T39Z55w"
      },
      "source": [
        "# thanks https://github.com/ltgoslo/simple_elmo\n",
        "from simple_elmo import ElmoModel as PretrainedElmoModel\n",
        "\n",
        "class ElmoModel(NeuralModel):\n",
        "  \"\"\"\n",
        "  An ELMO model. It doesn't need a tokenizer, so long as the input is already\n",
        "  tokenized.\n",
        "  Defaults to loading the bioreddit model.\n",
        "  \"\"\"\n",
        "  def __init__(self, model=None):\n",
        "    if not model:\n",
        "      self.__load_pretrained()\n",
        "      self.name = 'ELMo-br-1024'\n",
        "    else:\n",
        "      self.model = model\n",
        "      self.name = 'ELMo-1024'\n",
        "    self.d = 1024\n",
        "  \n",
        "  def __load_pretrained(self):\n",
        "    \"\"\"Load the Bioreddit ELMo model. Make sure you've downloaded it first!\"\"\"\n",
        "    self.use_simple_elmo = True\n",
        "    self.model = PretrainedElmoModel()\n",
        "    self.model.load('./elmo-br')\n",
        "  \n",
        "  def process_batch(self, batch, pooled=False):\n",
        "    \"\"\"\n",
        "    Process a single batch with ELMo.\n",
        "\n",
        "    Arguments:\n",
        "      batch: a list of strings to process.\n",
        "      pooled: average the word vectors.\n",
        "    \n",
        "    Returns:\n",
        "      if pooled, a tensor of shape (batch_size, 1024)\n",
        "      if not pooled, shape (batch_size, max_length, 1024)\n",
        "    \"\"\"\n",
        "    if self.use_simple_elmo:\n",
        "      embs = self.model.get_elmo_vector_average(batch) if pooled else self.model.get_elmo_vectors(batch)\n",
        "      return tf.constant(embs)\n",
        "\n",
        "    else:\n",
        "      if type(batch) == list:\n",
        "        batch = tf.constant(batch)\n",
        "    \n",
        "      embeds = self.model.signatures['default'](tf.constant(batch))\n",
        "      return embeds['default'] if pooled else embeds['word_emb']\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX6mVwM2By7O"
      },
      "source": [
        "### Static input models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J8MDiAabQR9"
      },
      "source": [
        "import gensim\n",
        "from scipy.fft import dct\n",
        "\n",
        "class StaticModel():\n",
        "  model_type = 'static'\n",
        "\n",
        "  def __dctransform(self, sentvec, max_k):\n",
        "    \"\"\"\n",
        "    Do a discrete cosine transformation on the embeddings.\n",
        "    See arxiv.org/pdf/1909.03104.pdf\n",
        "\n",
        "    From https://github.com/N-Almarwani/DCT_Sentence_Embedding/blob/master/DCT.py#L77\n",
        "\n",
        "    Parameters:\n",
        "      sentvec: the vectors for the individual words in a sentence.\n",
        "      max_k: the number of dcts to do.\n",
        "    \"\"\"\n",
        "    sentvec = sentvec.numpy()\n",
        "    if sentvec.shape[0] < max_k:\n",
        "      transformed = dct(sentvec, norm='ortho', n=max_k, axis=0)\n",
        "    else:\n",
        "      transformed = dct(sentvec, norm='ortho', axis=0)\n",
        "    embedding = np.reshape(transformed[:max_k,:], (max_k*sentvec.shape[1],))\n",
        "    return tf.constant(embedding)\n",
        "  \n",
        "  def process_string(self, string, pooled=True):\n",
        "    \"\"\"\n",
        "    Transform a single string into static vectors.\n",
        "    \n",
        "    Parameters:\n",
        "      string: the string to transform.\n",
        "      pooled: controls whether to pool the output by averaging or not.\n",
        "    \n",
        "    Returns a tensor of shape (d,), or (d,len(string)) if pooled is False.\n",
        "    \"\"\"\n",
        "    tokens = string.split() if pooled else string\n",
        "    out_of_vocab = np.zeros(self.d)\n",
        "\n",
        "    # Get each token's embedding\n",
        "    embs = [(self.model[token] if token in self.model else out_of_vocab) for token in tokens]  \n",
        "    embeddings = tf.stack(embs, axis=0)\n",
        "\n",
        "    if not pooled:\n",
        "      return embeddings\n",
        "    else:\n",
        "      return tf.reduce_mean(embeddings, axis=0)\n",
        "  \n",
        "  def process_dataset(self, dataset, pooled=True, pad_size=4):\n",
        "    \"\"\"\n",
        "    Process a whole dataset with static vectors.\n",
        "    Because Python lists are fast, there's no need for batches.\n",
        "\n",
        "    Parameters:\n",
        "      dataset: a list of strings to be processed.\n",
        "      pooled: if true, average the vectors for each word.\n",
        "      pad_size: the size to pad the vectors to. Only relevant if pooled=False.\n",
        "    \n",
        "    Returns: a tensor of static embeddings for the dataset.\n",
        "    \"\"\"\n",
        "    if not pooled: # truncate strings that are too long\n",
        "      dataset = [string.split()[0:pad_size] for string in dataset]\n",
        "    \n",
        "    outputs = [tf.dtypes.cast(self.process_string(string, pooled=pooled),\n",
        "                              tf.float64) for string in dataset]\n",
        "    if not pooled:\n",
        "      outputs = pad_outputs(outputs, min_size=pad_size, d=0, rank=2)\n",
        "    \n",
        "    return tf.stack(outputs, axis=0)\n",
        "  \n",
        "  def get_embeddings(self, *datasets, pooled=True, pad_size=4):\n",
        "    \"\"\"\n",
        "    Return all of the embeddings for a bunch of datasets.\n",
        "\n",
        "    Parameters:\n",
        "      datasets: a nonzero number of Dataset objects to be processed.\n",
        "      pooled: whether to average the embeddings for multi-word inputs.\n",
        "      pad_size: how much to pad the inputs to. Only relevant if pooled=False. \n",
        "\n",
        "    Returns a list of tensors, each containing the embeddings for a whole dataset.\n",
        "    \"\"\"\n",
        "    if type(self) == MixedModel:\n",
        "      embeddings = [self.process_dataset(ds,pooled=pooled) for ds in datasets]\n",
        "    else:\n",
        "      embeddings = [self.process_dataset(ds,pooled=pooled, pad_size=pad_size) for ds in datasets]\n",
        "    return embeddings\n",
        "\n",
        "class FTModel(StaticModel):\n",
        "  \"\"\"\n",
        "  A FastText model contained in the text file fname.\n",
        "  \"\"\"\n",
        "  def __init__(self, fname):\n",
        "    self.fname = fname\n",
        "    self.__load_model()\n",
        "\n",
        "  def __load_model(self):\n",
        "    self.name = 'ft-'+self.fname[0:-4]\n",
        "\n",
        "    print(\"Loading FastText model {}\".format(self.name))\n",
        "    self.model = gensim.models.KeyedVectors.load_word2vec_format(self.fname, limit=1000000)\n",
        "    self.d = self.model.vector_size\n",
        "\n",
        "    print(\"{} words loaded!\".format(len(self.model.vocab)))\n",
        "    print(\"d = {}\".format(self.d))\n",
        "\n",
        "class GloVeModel(StaticModel):\n",
        "  \"\"\"\n",
        "  A GloVe model contained in the text file fname.\n",
        "  \"\"\"\n",
        "  def __init__(self, fname):\n",
        "    self.fname = fname\n",
        "    self.__load_model()\n",
        "  \n",
        "  def __load_model(self):\n",
        "      \"\"\"\n",
        "      Load a single GloVe model.\n",
        "      Code from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
        "      \"\"\"\n",
        "      self.name = self.fname[0:-4]\n",
        "      print(\"Loading Glove Model {}\".format(self.name))\n",
        "      f = open(self.fname,'r')\n",
        "      self.model = {}\n",
        "\n",
        "      for line in f:\n",
        "        split_lines = line.split()\n",
        "        word = split_lines[0]\n",
        "        # this stops a really weird bug with glove.840B.300d\n",
        "        if split_lines[1] in {'.', 'name@domain.com', 'Killerseats.com', 'mylot.com', 'Amazon.com'}:\n",
        "          continue\n",
        "        \n",
        "        word_embedding = np.array([float(value) for value in split_lines[1:]])\n",
        "        self.model[word] = word_embedding\n",
        "      \n",
        "      self.d = len(self.model['a'])\n",
        "\n",
        "      print(\"{} words loaded!\".format(len(self.model)))\n",
        "      print(\"d = {}\".format(self.d))\n",
        "  \n",
        "class Node2VecModel(StaticModel):\n",
        "  \"\"\"\n",
        "  A Node2Vec model trained on the SNOMED knowledge graph.\n",
        "  See arxiv.org/abs/1907.08650\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    # drive location for n2v\n",
        "    self.fid = ''\n",
        "    self.name = 'n2v'\n",
        "    self.__load_model()\n",
        "  \n",
        "  def __load_model(self):\n",
        "    content = drive.CreateFile({'id': self.fid})\n",
        "    self.content = content.GetContentString()\n",
        "    \n",
        "    self.model = {}\n",
        "    for line in self.content.splitlines()[1:]:\n",
        "      split_lines = line.split()\n",
        "      id = split_lines[0]\n",
        "      id_embedding = np.array([float(value) for value in split_lines[1:]])\n",
        "      self.model[id] = id_embedding\n",
        "    \n",
        "    self.d = 200\n",
        "    print(\"{} ids loaded!\".format(len(self.model)))\n",
        "    print(\"d = {}\".format(self.d))\n",
        "  \n",
        "  def get_all(self):\n",
        "    \"\"\"Get all the SNOMED ids and all the SNOMED embeddings.\"\"\"\n",
        "    all_ids = list(self.model.keys())\n",
        "    all_embeds = list(self.model.values())\n",
        "\n",
        "    return all_ids, all_embeds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2a1S1F6E3fN"
      },
      "source": [
        "class MixedModel(StaticModel):\n",
        "  \"\"\"\n",
        "  A class for a model that is a concatenation of the outputs from two or more\n",
        "  models.\n",
        "\n",
        "  'Term' and 'term' ARE CASE SENSITIVE\n",
        "\n",
        "  dataset_cols: the columns of the dataset to be processed by each member of\n",
        "  models. len(models) == len(dataset_cols)\n",
        "  This can be one of:\n",
        "    - 'Term'\n",
        "    - 'Specific SNOMED ID'/'General SNOMED ID'\n",
        "    - 'Specific SNOMED Label'/'General SNOMED Label'\n",
        "    ===============\n",
        "    (Put a . between l and e for a separator)\n",
        "    - 'l e': label then example. Extract the label.\n",
        "    - 'e l': example then label. Extract the label.\n",
        "    - 'l t': label then term. Extract the label.\n",
        "    - 't l': term then label. Extract the label.\n",
        "     - 'term': Extract the term from the example.\n",
        "\n",
        "  For instance, you will need to pass the `Specific SNOMED ID` col to Node2Vec,\n",
        "  but you pass `Specific SNOMED Label` to FastText.\n",
        "  \"\"\"\n",
        "  def __init__(self, dataset_cols, submodels):\n",
        "    if len(dataset_cols) != len(submodels):\n",
        "      raise ValueError(\"Please pass one column per model, ta\")\n",
        "    self.dataset_cols = dataset_cols\n",
        "    self.submodels = submodels\n",
        "\n",
        "    self.name = \"+\".join([m.name for m in self.submodels])\n",
        "    self.d = sum([m.d for m in self.submodels])\n",
        "  \n",
        "  def process_dataset(self, dataset, pooled=True):\n",
        "    columns = []\n",
        "    for c in self.dataset_cols:\n",
        "      if re.match(\"((General|Specific)\\ SNOMED\\ (ID|Label))|Term\", c):\n",
        "        columns.append(c)\n",
        "      else:\n",
        "        columns.append('')\n",
        "    \n",
        "    embeds = []\n",
        "    # process each column with the appropriate dataset\n",
        "    for i in range(0, len(self.dataset_cols)):\n",
        "      col = self.dataset_cols[i]\n",
        "      submodel = self.submodels[i]\n",
        "      if re.match(\"((General|Specific)\\ SNOMED\\ (ID|Label))|Term\", col):\n",
        "        # context-insensitive embeddings\n",
        "        column = dataset.col(columns[i])\n",
        "        embeds.append(submodel.process_dataset(column, pooled=pooled))\n",
        "      else:\n",
        "        # context-sensitive embeddings\n",
        "        # get_cs_embeddings processes whole datasets, so it returns a list with\n",
        "        # one element here\n",
        "        embeds.append(submodel.get_cs_embeddings(dataset, order=col[-3:], pooled=pooled)[0])\n",
        "\n",
        "    if pooled:\n",
        "      return tf.concat(embeds, -1)\n",
        "    else:\n",
        "      # embeds is a list of shapes [(batch_size, pad_len0, d0), (batch_size, pad_len1, d1), ...]\n",
        "      # so we need to pad all of the tensors to a uniform pad length\n",
        "      padded_embeds = pad_outputs(embeds, min_size=None, d=1, batches=False)\n",
        "      return tf.concat(padded_embeds, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLYx_nWLjybi"
      },
      "source": [
        "### Download static models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6Di3MHYCgNT"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip # wiki+gigaword5\n",
        "# !wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "# !wget http://nlp.stanford.edu/data/glove.42B.300d.zip # common crawl\n",
        "# !wget http://nlp.stanford.edu/data/glove.840B.300d.zip # common crawl\n",
        "# !wget https://github.com/basaldella/bioreddit/releases/download/v1.0/bioreddit.glove.100.txt\n",
        "# !wget https://github.com/basaldella/bioreddit/releases/download/v1.0/bioreddit.glove.200.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj9QH4wvZW3w"
      },
      "source": [
        "# !unzip glove.6B.zip\n",
        "# !rm glove.6B.zip\n",
        "# !unzip glove.42B.300d\n",
        "# !rm glove.42B.300d.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhJtTrEU2oo7"
      },
      "source": [
        "# Download the FastTexts\n",
        "\n",
        "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip\n",
        "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\n",
        "# !gdown https://drive.google.com/uc?id=1CTZEO9pvR3C8DbxJ7bt-y4t3TWGirQ_0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HHHXkegfQxC"
      },
      "source": [
        "# !unzip wiki-news-300d-1M-subword.vec\n",
        "# !rm wiki-news-300d-1M-subword.vec.zip\n",
        "# !unzip crawl-300d-2M-subword\n",
        "# !rm crawl-300d-2M-subword.bin\n",
        "# !rm crawl-300d-2M-subword.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrCgfTA7Zm0z"
      },
      "source": [
        "### Load static models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFFm7mhLBxtf"
      },
      "source": [
        "def load_static_models(models, model_type):\n",
        "  \"\"\"\n",
        "  Load all of the static models of a particular model_type (GloVe or FastText).\n",
        "  Returns a dictionary of models. Each one has the same name as the filename,\n",
        "  but without the .txt or .vec.\n",
        "  \"\"\"\n",
        "  if model_type not in ['fasttext', 'glove']:\n",
        "    raise ValueError(\"Only 'fasttext' or 'glove' please.\")\n",
        "  \n",
        "  static_models = {}\n",
        "  for model in models:\n",
        "    if os.path.exists(model):\n",
        "      model_name = model[0:-4]\n",
        "      #static_models[model_name] = load_glove_model(model) if model_type == 'glove' else load_ft_model(model)\n",
        "      static_models[model_name] = GloVeModel(model) if model_type == 'glove' else FTModel(model)\n",
        "  \n",
        "  return static_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek1pd3XohWLn"
      },
      "source": [
        "glove_models = ['glove.6B.300d.txt', 'glove.6B.200d.txt', 'glove.6B.100d.txt',\n",
        "                'glove.42B.300d.txt',\n",
        "                'glove.twitter.27B.200d.txt', 'glove.twitter.27B.100d.txt',\n",
        "                'bioreddit.glove.200.txt', 'bioreddit.glove.100.txt']\n",
        "fasttext_models = ['wiki-news-300d-1M-subword.vec', 'crawl-300d-2M-subword.vec',\n",
        "                   'reddit-biomed.vec']\n",
        "\n",
        "loaded_glove_models = load_static_models(glove_models, 'glove')\n",
        "loaded_ft_models = load_static_models(fasttext_models, 'fasttext')\n",
        "snomed2vec = Node2VecModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVna1JHh44qa"
      },
      "source": [
        "### Linear experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y57OYDBtmvMg"
      },
      "source": [
        "class LinearExperiment(Experiment):\n",
        "  \"\"\"\n",
        "  A class with which to run an experiment for a linear mapping between terms\n",
        "  and labels.\n",
        "\n",
        "  Parameters:\n",
        "  \n",
        "  in_model: a StaticModel or NeuralModel object that produces the\n",
        "  inputs to the transformation (i.e. the model that embeds the terms)\n",
        "\n",
        "  out_model: the same, but the mdoel that embeds the concepts.\n",
        "\n",
        "  datasets: a list of two Dataset objects in the order train, test.\n",
        "  The linear mapping will be trained on the train dataset and evaluated against\n",
        "  the test dataset.\n",
        "  \n",
        "  k_values: a list of the values of k to try.\n",
        "\n",
        "  context_sensitive can be:\n",
        "    '': neither the labels nor the terms are embedded context-sensitively\n",
        "    'l e': label then example. Extract the label.\n",
        "    'e l': example then label. Extract the label.\n",
        "    'l t': label then term. Extract the label.\n",
        "    't l': term then label. Extract the label.\n",
        "    'term': Extract the term from the example.\n",
        "  You can combine these as e.g. 'lt-term', which gives you context-sensitive\n",
        "  labels and terms.\n",
        "  \"\"\"\n",
        "  def __init__(self, in_model, out_model, datasets, k_values, context_sensitive=''):\n",
        "    if in_model.d != out_model.d:\n",
        "      raise ValueError(\"The two models need the same dimensions I'm afraid.\")\n",
        "    self.in_model = in_model; self.out_model = out_model\n",
        "\n",
        "    self.use_ids = False\n",
        "    if self.out_model.name == 'snomed2vec-200':\n",
        "      self.use_ids = True\n",
        "    self.datasets = datasets\n",
        "    self.pooled = True # compatibility\n",
        "    \n",
        "    self.train_out = datasets[0].ids if self.use_ids else datasets[0].labels\n",
        "    self.train_out_other = datasets[0].labels if self.use_ids else datasets[0].ids\n",
        "    self.train_in = datasets[0].terms\n",
        "\n",
        "    self.test_out = datasets[1].ids if self.use_ids else datasets[1].labels\n",
        "    self.test_out_other = datasets[1].labels if self.use_ids else datasets[1].ids\n",
        "    self.test_in = datasets[1].terms\n",
        "\n",
        "    self.k_values = k_values\n",
        "    self.cs = context_sensitive\n",
        "    self.description = 'linear'\n",
        "\n",
        "  def __generate_embeds(self):\n",
        "    \"\"\"\n",
        "    Generate the embeddings for the train_out, train_in, test_in, test_out\n",
        "    datasets.\n",
        "    \"\"\"\n",
        "    if not self.cs:\n",
        "      self.in_name = self.in_model.name\n",
        "      self.x_train, self.x_test = self.in_model.get_embeddings(\n",
        "          self.train_in, self.test_in, pooled=True\n",
        "      )\n",
        "      self.out_name = self.out_model.name\n",
        "      self.y_train, self.y_test = self.out_model.get_embeddings(\n",
        "          self.train_out, self.test_out, pooled=True\n",
        "      )\n",
        "    else:\n",
        "      out_order = self.cs[0:3]\n",
        "      self.out_name = self.out_model.name + '-cs_' + out_order\n",
        "      self.y_train, self.y_test = self.out_model.get_cs_embeddings(\n",
        "          self.datasets[0], self.datasets[1], order=out_order, pooled=True\n",
        "      )\n",
        "      if self.cs[-4:] == 'term':\n",
        "        self.in_name = self.in_model.name + '-cs_term'\n",
        "        self.x_train, self.x_test = self.in_model.get_cs_embeddings(\n",
        "            self.datasets[0], self.datasets[1], order='term',\n",
        "        )\n",
        "      else:\n",
        "        self.in_name = self.in_model.name\n",
        "        self.x_train, self.x_test = self.in_model.get_embeddings(\n",
        "          self.train_in, self.test_in, pooled=True\n",
        "      )\n",
        "\n",
        "  def learn_transformation(self, source, target):\n",
        "    \"\"\"\n",
        "    Learn a svd transformation between source and target.\n",
        "    `source` and `target` should both have the shape (n, d) where n is the number\n",
        "    of training examples.\n",
        "    From https://github.com/babylonhealth/fastText_multilingual/blob/master/align_your_own.ipynb\n",
        "    see https://arxiv.org/pdf/1702.03859.pdf\n",
        "    \"\"\"\n",
        "    source = np.array(source)\n",
        "    target = np.array(target)\n",
        "\n",
        "    product = np.matmul(source.transpose(), target)\n",
        "    U, Sigma, V_transpose = np.linalg.svd(product)\n",
        "\n",
        "    return np.matmul(U, V_transpose)\n",
        "\n",
        "  def create_mapping(self):\n",
        "    \"\"\"A wrapper to create the linear mapping.\"\"\"\n",
        "    self.__generate_embeds()\n",
        "    #self.x_train = tf.make_ndarray(self.x_train)\n",
        "    #self.y_train = tf.make_ndarray(self.y_train)\n",
        "    self.transformation = self.learn_transformation(self.x_train, self.y_train)\n",
        "\n",
        "  def __apply_transformation(self, embedding):\n",
        "    \"\"\"Apply the transformation to an embedding.\"\"\"\n",
        "    embedding = np.array(embedding)\n",
        "    return np.matmul(embedding, self.transformation)\n",
        "  \n",
        "  def make_predictions(self):\n",
        "    \"\"\"A wrapper to get the model to make predictions.\"\"\"\n",
        "    #self.predictions = [self.__apply_transformation(embed) for embed in tf.make_ndarray(self.x_test)]\n",
        "    self.predictions = [self.__apply_transformation(embed) for embed in self.x_test]\n",
        "  \n",
        "  def metrics(self):\n",
        "    # compatibility\n",
        "    return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFsAv3tLpKdi"
      },
      "source": [
        "class LinearExperiments(Experiments):\n",
        "  \"\"\"\n",
        "  A class to represent several linear mapping experiments run with several\n",
        "  different in_models and several different values of k. They should all use the\n",
        "  same datasets.\n",
        "\n",
        "  Parameters:\n",
        "\n",
        "  in_models: a list of StaticModel and NeuralModel objects. These\n",
        "  produce the inputs to the linear mapping. Each in_model will be evaluated\n",
        "  against each value of k.\n",
        "\n",
        "  out_models: the same, but for the outputs.\n",
        "\n",
        "  cs_values: a list of the context-sensitive values for each model.\n",
        "  \n",
        "  datasets: a list of two Dataset objects, in the order train, test.\n",
        "  The same datasets will be used for each model--k_value pair.\n",
        "\n",
        "  k_values: a list of values of k to try each model against.\n",
        "  \"\"\"\n",
        "  def __init__(self, in_models, out_models, cs_values, datasets, k_values):\n",
        "    if not len(in_models) == len(out_models) == len(cs_values):\n",
        "      raise ValueError(\"One input model for one output model for one cs_value, ta\")\n",
        "    self.in_models = in_models\n",
        "    self.out_models = out_models\n",
        "    self.datasets = datasets\n",
        "    self.k_values = k_values\n",
        "    self.cs_values = cs_values\n",
        "    \n",
        "    # header for the data output\n",
        "    self.header = ['in_model', 'd_in', 'out_model', 'd_out', 'k', 'Correct (all)',\n",
        "                   'n', 'Correct (ee)', 'n (ee)', 'avg_edit (norm\\'d)', \"\"]\n",
        "\n",
        "    self.experiments = []\n",
        "    for i in range(0, len(in_models)):\n",
        "      in_model = in_models[i]\n",
        "      out_model = out_models[i]\n",
        "      cs = cs_values[i]\n",
        "      self.experiments.append(LinearExperiment(in_model, out_model, datasets,\n",
        "                                               k_values, context_sensitive=cs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJaSxUtEeqX_"
      },
      "source": [
        "datasets = [train_d, test_d, val_d]\n",
        "datasets_zsh = [train_d_zsh, test_d_zsh, val_d_zsh]\n",
        "bert = BertModel(bert_tokenizer, bert_model, 'bert_large', 'lasths')\n",
        "\n",
        "in_models = [bert]\n",
        "out_models = in_models\n",
        "cs_values = ['']\n",
        "exps = LinearExperiments(in_models, out_models, cs_values, datasets, [1,10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDkZw80ee60d"
      },
      "source": [
        "exps.run(zsh=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M7BC2jp9asX"
      },
      "source": [
        "### Neural experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJO24qKdVQob"
      },
      "source": [
        "import IPython\n",
        "\n",
        "class NeuralExperiment(Experiment):\n",
        "  \"\"\"\n",
        "  A class with which to run an experiment for a neural mapping between terms\n",
        "  and labels.\n",
        "\n",
        "  Parameters:\n",
        "\n",
        "  layers: the layers to use in the neural network architecture. It should\n",
        "  be a list of keras layers. It should NOT include the input layer nor the final\n",
        "  layer.\n",
        "\n",
        "  arch_type: the type of architecture used (ffnn, CNN, RNN, etc).\n",
        "    - CNN: use the format 'cnn-4' where 4 is the size of the convolution window.\n",
        "    - LSTM: use 'lstm'\n",
        "    - GRU: use 'gru'\n",
        "  \n",
        "  in_model: a StaticModel or NeuralModel object that produces the\n",
        "  inputs to the neural model (i.e. the model that embeds the terms)\n",
        "\n",
        "  out_model: the same, but the mdoel that embeds the concepts.\n",
        "\n",
        "  datasets: a list of three Dataset objects in the order [train, test,\n",
        "  val]. The neural mapping will be trained on the train dataset and evaluated\n",
        "  against the test dataset, with the val dataset for tuning.\n",
        "\n",
        "  k_values: a list of the values of k to try.\n",
        "\n",
        "  context_sensitive can be:\n",
        "    - '': neither the labels nor the terms are embedded context-sensitively\n",
        "    - 'term': Extract the term from the example.\n",
        "    (Below, put a . between l and e for a separator)\n",
        "    - 'l e': label then example. Extract the label.\n",
        "    - 'e l': example then label. Extract the label.\n",
        "    - 'l t': label then term. Extract the label.\n",
        "    - 't l': term then label. Extract the label.\n",
        "  You can combine these as e.g. 'lt-term', which gives you context-sensitive\n",
        "  labels and terms.\n",
        "\n",
        "  use_ids: whether the SNOMED id is used. If it is False, then the natural\n",
        "  language description of the SNOMED concept is used.\n",
        "  \"\"\"\n",
        "  def __init__(self, layers, arch_type, in_model, out_model, datasets, k_values,\n",
        "               context_sensitive='', use_ids=False):\n",
        "    self.layers = layers\n",
        "\n",
        "    if arch_type[0:3] == 'cnn' or arch_type[-4:] == 'lstm' or arch_type == 'gru':\n",
        "      self.arch_type = arch_type\n",
        "      self.pooled = False\n",
        "      self.kernel_size = int(arch_type[4:]) if self.arch_type[0:3] == 'cnn' else None\n",
        "    elif arch_type[0:4] == 'ffnn':\n",
        "      self.arch_type = arch_type\n",
        "      self.pooled = True\n",
        "\n",
        "    self.use_ids = use_ids\n",
        "\n",
        "    self.in_model = in_model\n",
        "    self.out_model = out_model\n",
        "    self.d_in = self.in_model.d\n",
        "    self.d_out = self.out_model.d\n",
        "    \n",
        "    self.mixedin = True if isinstance(self.in_model, MixedModel) else False\n",
        "\n",
        "    self.mixedout = True if isinstance(self.out_model, MixedModel) else False\n",
        "\n",
        "    if type(self.out_model) == Node2VecModel:\n",
        "      # node2vec has to use the SNOMED ids\n",
        "      self.use_ids = True\n",
        "\n",
        "    self.datasets = datasets\n",
        "    self.cs = context_sensitive\n",
        "    self.k_values = k_values\n",
        "    \n",
        "    # _out_other is used for analysing the results\n",
        "    self.train_out = datasets[0].ids if self.use_ids else datasets[0].labels\n",
        "    self.train_out_other = datasets[0].labels if self.use_ids else datasets[0].ids\n",
        "    self.train_in = datasets[0].terms\n",
        "\n",
        "    self.test_out = datasets[1].ids if self.use_ids else datasets[1].labels\n",
        "    self.test_out_other = datasets[1].labels if self.use_ids else datasets[1].ids\n",
        "    self.test_in = datasets[1].terms\n",
        "\n",
        "    self.val_out = datasets[2].ids if self.use_ids else datasets[2].labels\n",
        "    self.val_in = datasets[2].terms\n",
        "    self.val_out_other = datasets[2].labels if self.use_ids else datasets[2].ids\n",
        "\n",
        "  def __generate_inembeds(self):\n",
        "    \"\"\"Create the input embeddings.\"\"\"\n",
        "    # I apologise for how complicated the logic is here.\n",
        "    self.in_name = self.in_model.name\n",
        "    if self.mixedin:\n",
        "      self.x_train, self.x_test, self.x_val = self.in_model.get_embeddings(\n",
        "          self.datasets[0], self.datasets[1], self.datasets[2], pooled=self.pooled)\n",
        "    else:\n",
        "      if self.cs[-4:] == 'term': # context-sensitive term embeddings\n",
        "        self.in_name += '-cs_term'\n",
        "        embs = self.in_model.get_cs_embeddings(*self.datasets, order='term', pooled=True)\n",
        "      else: # context-insensitive term embeddings\n",
        "        strings = [self.train_in, self.test_in, self.val_in]\n",
        "        if not self.pooled:\n",
        "          embs = self.in_model.get_embeddings(*strings, pooled=False, pad_size=8)\n",
        "        else:\n",
        "          embs = self.in_model.get_embeddings(*strings, pooled=True)\n",
        "    \n",
        "      self.x_train, self.x_test, self.x_val = embs[0], embs[1], embs[2]\n",
        "\n",
        "  def __generate_outembeds(self):\n",
        "    \"\"\"Generate the output emeddings.\"\"\"\n",
        "    # TODO: Make this more elegant by passing use_ids to self.out_model.get_embeddings\n",
        "    self.out_name = self.out_model.name\n",
        "    if self.mixedout:\n",
        "      self.y_train, self.y_test, self.y_val = self.out_model.get_embeddings(\n",
        "          self.datasets[0], self.datasets[1], self.datasets[2])\n",
        "    elif self.cs not in ['', '-term']:\n",
        "      order = self.cs[0:3]\n",
        "      self.y_train, self.y_test, self.y_val = self.out_model.get_cs_embeddings(\n",
        "          self.datasets[0], self.datasets[1], self.datasets[2], order=order, pooled=True,\n",
        "      )\n",
        "      self.out_name += '-cs_' + order\n",
        "    else:\n",
        "      strings = [self.train_out, self.val_out, self.test_out]\n",
        "      self.y_train, self.y_val, self.y_test = self.out_model.get_embeddings(*strings, pooled=True)\n",
        "      \n",
        "  def create_mapping(self):\n",
        "    \"\"\"A wrapper to create the neural mapping.\"\"\"\n",
        "    # Create the embeddings\n",
        "    self.__generate_inembeds()\n",
        "    self.__generate_outembeds()\n",
        "\n",
        "    self.train_dataset = tf.data.Dataset.from_tensor_slices((self.x_train, self.y_train))\n",
        "    self.train_dataset = self.train_dataset.batch(64)\n",
        "    self.val_dataset = tf.data.Dataset.from_tensor_slices((self.x_val, self.y_val))\n",
        "    self.val_dataset = self.val_dataset.batch(64)\n",
        "\n",
        "    # Create the model\n",
        "    if self.arch_type == 'ffnn':\n",
        "      self.create_ffnn()\n",
        "    elif self.arch_type == 'cnn':\n",
        "      self.create_cnn()\n",
        "    elif self.arch_type == 'lstm':\n",
        "      self.create_lstm()\n",
        "    elif self.arch_type == 'bilstm':\n",
        "      self.create_bilstm()\n",
        "    elif self.arch_type == 'gru':\n",
        "      self.create_gru()\n",
        "    \n",
        "    # Describe the model\n",
        "    self.optimizer = 'adam-0.001-50'\n",
        "    self.description = self.optimizer+'-'+self.in_model.name\n",
        "    if self.pooled:\n",
        "      self.description += '-avg-'\n",
        "    else:\n",
        "      self.description += '-'\n",
        "\n",
        "    self.description += '-'.join([layer.name for layer in self.model.layers])\n",
        "    self.description += '-'+self.out_model.name\n",
        "\n",
        "    # Compile the model\n",
        "    self.model.compile(optimizer='adam', # adam works better\n",
        "                       loss='mse',\n",
        "                       metrics=['cosine_similarity']) # cos-similarity between y and \n",
        "\n",
        "    print(self.description)\n",
        "    print(self.model.summary())\n",
        "    \n",
        "    self.history = self.model.fit(self.train_dataset, verbose=0, batch_size=64,\n",
        "                                  epochs=50, validation_data=self.val_dataset)\n",
        "    \n",
        "    return self.model, self.history\n",
        "\n",
        "  def create_ffnn(self):\n",
        "    \"\"\"Train a feedforward neural network model.\"\"\"\n",
        "    self.model = keras.Sequential([\n",
        "      keras.Input(shape=(self.d_in,)),\n",
        "      *self.layers,\n",
        "      keras.layers.Dense(self.d_out, activation='relu', name='dense_out-'+str(self.d_out)),\n",
        "    ])\n",
        "\n",
        "  def create_lstm(self):\n",
        "    \"\"\"Train a LSTM network.\"\"\"\n",
        "    in_size = self.x_train.shape[1]\n",
        "\n",
        "    self.model = keras.Sequential([\n",
        "      keras.Input(shape=(in_size, self.d_in,)),\n",
        "      keras.layers.LSTM(self.d_out, name='lstm-'+str(self.d_out)),\n",
        "      keras.layers.BatchNormalization(name='batchnorm')\n",
        "    ])\n",
        "\n",
        "  def create_bilstm(self):\n",
        "    \"\"\"Train a LSTM network.\"\"\"\n",
        "    in_size = self.x_train.shape[1]\n",
        "\n",
        "    self.model = keras.Sequential([\n",
        "      keras.Input(shape=(in_size, self.d_in,)),\n",
        "      keras.layers.Bidirectional(\n",
        "        keras.layers.LSTM(self.d_out, name='bilstm-'+str(2*self.d_out)),\n",
        "        merge_mode='concat',\n",
        "      ),\n",
        "      keras.layers.BatchNormalization(name='batchnorm'),\n",
        "      keras.layers.Dense(self.d_out, activation='relu', name='dense_out'+str(self.d_out)),\n",
        "    ])\n",
        "\n",
        "  def create_gru(self):\n",
        "    \"\"\"Train a GRU network.\"\"\"\n",
        "    in_size = self.x_train.shape[1]\n",
        "\n",
        "    self.model = keras.Sequential([\n",
        "      keras.Input(shape=(in_size, self.d_in,)),\n",
        "      keras.layers.GRU(self.d_out, name='gru-'+str(self.d_out)),\n",
        "    ])\n",
        "\n",
        "  def create_cnn(self):\n",
        "    \"\"\"Train a CNN model.\n",
        "\n",
        "    The input (self.x_train etc) will be tensors of size (batch, in_size, d_in)\n",
        "    The output (self.y_train etc) will be of the size (batch, d_out).\n",
        "    Outputs can NEVER be of rank 3.\n",
        "    \"\"\"\n",
        "    in_size = self.x_train.shape[1]\n",
        "\n",
        "    self.conv_layer = self.layers[0]\n",
        "    self.pool_layer = self.layers[1]\n",
        "\n",
        "    self.model = keras.Sequential([\n",
        "      keras.Input(shape=(in_size, self.d_in,)),\n",
        "      keras.layers.Conv1D(128, self.kernel_size, activation='relu',\n",
        "                          input_shape=self.x_train.shape[1:], name='conv1d-128-'+str(self.kernel_size)),\n",
        "      keras.layers.GlobalMaxPool1D(name='globalmaxpool1d'), # pool across axis -2\n",
        "      *self.layers,\n",
        "      keras.layers.Dense(self.d_out, activation='relu', name='dense_out-'+str(self.d_out)),\n",
        "    ])\n",
        "\n",
        "  def make_predictions(self):\n",
        "    \"\"\"A wrapper to get the model to make predictions.\"\"\"\n",
        "    self.predictions = self.model.predict(self.x_test)\n",
        "  \n",
        "  def metrics(self):\n",
        "    \"\"\"\n",
        "    Return a list of metrics:\n",
        "    - the cosine similarity metric, evaluated on the test set.\n",
        "    - the number of params for each model\"\"\"\n",
        "    metrics = [str(self.model.evaluate(self.x_test, self.y_test, return_dict=True)['cosine_similarity']),\n",
        "               str(self.model.count_params())\n",
        "    ]\n",
        "    return metrics\n",
        "  \n",
        "  def plot(self, save=True):\n",
        "    \"\"\"Plot a pretty graph of the learning curve, saving it if needed.\"\"\"\n",
        "    #description = self.optimizer+'-'+self.in_model.name+'-'+'-'.join([layer.name for layer in self.model.layers])\n",
        "    fname = './drive/MyDrive/Dissertation/'+ self.description + '.png'\n",
        "    print(self.description)\n",
        "\n",
        "    plt.plot(self.history.history['cosine_similarity'])\n",
        "    plt.plot(self.history.history['val_cosine_similarity'])\n",
        "    plt.ylim(ymin=0)\n",
        "    plt.ylabel('Cosine Similarity')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "    if save:\n",
        "      plt.savefig(fname, dpi=300)\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "  def tune_ffnn(self, nlayers=3):\n",
        "    \"\"\"Tune a feedforward neural model, with nlayers.\"\"\"\n",
        "    self.__generate_inembeds()\n",
        "    self.__generate_outembeds()\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((self.x_train, self.y_train))\n",
        "    train_dataset = train_dataset.batch(64)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((self.x_val, self.y_val))\n",
        "    val_dataset = val_dataset.batch(64)\n",
        "\n",
        "    def model_builder(hp):\n",
        "      hp_layers = []; layers = []\n",
        "      for i in range(0,nlayers):\n",
        "        hp_layer = hp.Int('units_'+str(i), min_value = 100, max_value = 3000, step = 50)\n",
        "        hp_layers.append(hp_layer)\n",
        "        layers.append(keras.layers.Dense(hp_layer, activation='relu', name='dense_'+str(i)))\n",
        "\n",
        "      # hp_units_0 = hp.Int('units_0', min_value = 100, max_value = 3000, step = 50)\n",
        "      # hp_units_1 = hp.Int('units_1', min_value = 100, max_value = 3000, step = 50)\n",
        "      # hp_units_2 = hp.Int('units_2', min_value = 100, max_value = 3000, step = 50)\n",
        "\n",
        "      model = keras.Sequential([\n",
        "        keras.Input(shape=(self.d_in,)),\n",
        "        *layers,\n",
        "        keras.layers.Dense(self.d_out, activation='relu', name='dense_out-'+str(self.d_out)),\n",
        "      ])\n",
        "\n",
        "      hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])\n",
        "\n",
        "      model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
        "                    loss = keras.losses.MeanSquaredError(), \n",
        "                    metrics = ['cosine_similarity'])\n",
        "      return model\n",
        "    \n",
        "    pname = self.in_model.name+'-'+str(nlayers)+'-hidden'\n",
        "    tuner = kt.Hyperband(\n",
        "      model_builder,\n",
        "      objective = kt.Objective('val_cosine_similarity', direction=\"max\"),\n",
        "      max_epochs = 10,\n",
        "      directory = './'+pname,\n",
        "      project_name = pname\n",
        "    )\n",
        "\n",
        "    class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
        "      def on_train_end(*args, **kwargs):\n",
        "        IPython.display.clear_output(wait = True)\n",
        "\n",
        "    tuner.search(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[ClearTrainingOutput()])\n",
        "\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "    optimal = []\n",
        "    for i in range(0, nlayers):\n",
        "      optimal.append(best_hps.get('units_'+str(i)))\n",
        "      print(\"Layer \"+str(i)+\": \"+str(best_hps.get('units_'+str(i))))\n",
        "    optimal.append(best_hps.get('learning_rate'))\n",
        "    print(\"Learning rate: \"+str(best_hps.get('learning_rate')))\n",
        "    \n",
        "    return optimal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezMpeEWUzW2l"
      },
      "source": [
        "class NeuralExperiments(Experiments):\n",
        "  \"\"\"\n",
        "  A class to represent several neural mapping experiments run with several\n",
        "  different in_models and several different values of k. They should all use the\n",
        "  same datasets.\n",
        "\n",
        "  Parameters:\n",
        "\n",
        "  architectures: a list of the architectures to try. Each architecure\n",
        "  should be a list of keras layers. They should NOT include the input layer or\n",
        "  the final layer.\n",
        "\n",
        "  arch_types: a list of architecture types (MLP, CNN, RNN etc). It\n",
        "  should be a list of the same length as `architectures`.\n",
        "\n",
        "  in_models: a list of a StaticModel or NeuralModel objects. These produce the\n",
        "  inputs to the experiments. Each in_model will be evaluated against each value of k.\n",
        "\n",
        "  out_models: the same, but for the output.\n",
        "\n",
        "  cs_values: a list of values of cs (context_sensitive) for each model.\n",
        "\n",
        "  datasets: a list of three Dataset objects, in the order train, test, val.\n",
        "  The same datasets will be used for each model--k_value pair.\n",
        "  \n",
        "  k_values: a list of values of k to try each model against.\n",
        "  \"\"\"\n",
        "  def __init__(self, architectures, arch_types, in_models, out_models, cs_values, datasets, k_values):\n",
        "    if len(in_models) != len(out_models):\n",
        "      raise ValueError(\"Please have the same number of input models as output models.\")\n",
        "    self.in_models = in_models\n",
        "    self.out_models = out_models\n",
        "    if len(architectures) != len(arch_types):\n",
        "      raise ValueError(\"Please match every architecture with an architecture type, tedious though it is.\")\n",
        "    \n",
        "    self.architectures = architectures\n",
        "    self.arch_types = arch_types\n",
        "    self.cs_values = cs_values\n",
        "    self.datasets = datasets\n",
        "    self.k_values = k_values\n",
        "\n",
        "    # header to use with the results dataset\n",
        "    self.header = ['in_model', 'd_in', 'out_model', 'd_out', 'k', 'Correct (all)',\n",
        "                   'n', 'Correct (ee)', 'n (ee)', 'avg_edit (norm\\'d)', 'cos_sim',\n",
        "                   'params', 'description']\n",
        "    \n",
        "    self.experiments = []\n",
        "    for i in range(0, len(in_models)):\n",
        "      in_model = in_models[i]\n",
        "      out_model = out_models[i]\n",
        "      arch = self.architectures[i]\n",
        "      arch_type = self.arch_types[i]\n",
        "      cs_val = self.cs_values[i]\n",
        "\n",
        "      self.experiments.append(NeuralExperiment(arch, arch_type, in_model, out_model,\n",
        "                                               datasets, self.k_values, cs_val))\n",
        "\n",
        "  def plot(self, save=True):\n",
        "    for experiment in self.experiments:\n",
        "      experiment.plot(save=save)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8AYl-y6Kkcf"
      },
      "source": [
        "# Load the input and output language models\n",
        "\n",
        "fasttext = loaded_ft_models['reddit-biomed']; fasttext.name = 'ft_br'\n",
        "bert = BertModel(bert_tokenizer, bert_model, 'bert_br', 'lasths')\n",
        "\n",
        "bertftnode2vec_out = MixedModel(['Specific SNOMED ID', 'Specific SNOMED Label', 'Specific SNOMED Label'],\n",
        "                                [snomed2vec, fasttext, bert])\n",
        "ftnode2vec_out = MixedModel(['Specific SNOMED ID', 'Specific SNOMED Label'],\n",
        "                            [snomed2vec, fasttext])\n",
        "bertnode2vec_out = MixedModel(['Specific SNOMED ID', 'Specific SNOMED Label'],\n",
        "                              [snomed2vec, bert])\n",
        "\n",
        "bertft_in = MixedModel(['Term', 'Term'], [fasttext, bert])\n",
        "bertft_out = MixedModel(['Specific SNOMED Label', 'Specific SNOMED Label'],\n",
        "                        [fasttext, bert])\n",
        "\n",
        "bertft_out_cs1 = MixedModel(['Specific SNOMED Label', 'l t'], [fasttext, bert])\n",
        "bertft_out_cs2 = MixedModel(['Specific SNOMED Label', 'l.t'], [fasttext, bert])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrDwzoEXx4ia"
      },
      "source": [
        "# for some reason you can't just do [[arch]]*3, you've got to write out\n",
        "# [[arch], [arch], [arch]] manually... I think it has something to do with the\n",
        "# way Python handles lists???\n",
        "archs = [\n",
        "[\n",
        "          keras.layers.Dense(500, activation='relu', name='dense_0-500'),\n",
        "          keras.layers.Dense(500, activation='relu', name='dense_1-500'),\n",
        "          keras.layers.Dense(500, activation='relu', name='dense_2-500'),\n",
        "],\n",
        "] \n",
        "\n",
        "arch_types = ['ffnn'] *1\n",
        "in_models = [bert]\n",
        "out_models = [bert]\n",
        "datasets = [train_d, test_d, val_d]\n",
        "datasets_zsh = [train_d_zsh, test_d_zsh, val_d_zsh]\n",
        "k_values = [1, 10]\n",
        "cs_values = ['']\n",
        "\n",
        "exps_strat = NeuralExperiments(archs, arch_types, in_models, out_models, cs_values, datasets, k_values)\n",
        "exps_zsh = NeuralExperiments(archs, arch_types, in_models, out_models, cs_values, datasets_zsh, k_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKjAMoY40VcV"
      },
      "source": [
        "exps_strat.run(zsh=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6se3ZOCEKIQy"
      },
      "source": [
        "exps_zsh.run(zsh=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25zAXoEF0suZ"
      },
      "source": [
        "exps_zsh.plot(save=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}